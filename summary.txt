# 데이터 타입 출력
df1.dtypes

================================================================================================
cat_feat = df.select_dtypes('object', 'category').columns.values
print(cat_feat)
df_cat = df.copy()

df_cat['Department'].value_counts()

# 3번 문제
# 데이터셋에서 숫자형인 칼럼만 추출하여 새로운 데이터 프레임 생성하고, 각 변수간의 상관계수를 구하고, 0.9인 이상 두 개의 칼럼 찾아내어 그 중 변수하나 제거
num_feat = df.select_dtypes('number').columns.values
df_num = df[num_feat].copy()

df_num.corr()
================================================================================================

# 데이터 타입 변경 1개
df1 = df.copy() # copy() 사용 안하면 안됨
df1 = df1.astype({'칼럼명' : '자료형'})

# 데이터 기초통계량 확인
df.describe(include='category') # category 타입 변수 기초통계량 확인

# 기초 통계량 구하기
mpg_mean = df['mpg'].mean()
mpg_median = df['mpg'].median()
mpg_mode = df['mpg'].mode()

# 산포도를 나타내는 값
mpg_var = df['mpg'].var()
mpg_std = df['mpg'].std()
# IQR
Q1 = df['mpg'].quantile(.25)
Q3 = df['mpg'].quantile(.75)
IQR = Q3 - Q1
Q2 = df['mpg'].quantile(.50) = median 값과 동일

# 범위(range) = 최대값 - 최소값
mpg_max = df['mpg'].max()
mpg_min = df['mpg'].min()
mpg_range = mpg_max - mpg_min

# 왜도: 좌우로 비대칭 // 양수면 양의 왜도(평균>중앙값>최빈값), 음수면 음의 왜도(평균<중앙값<최빈값)
mpg_skew = df['mpg'].skew()

# 첨도: 위아래로 뾰족한지
mpg_kurt = df['mpg'].kurt()

# 그룹화하여 계산하기
df.groupby('species').mean() : species의 종별로 평균값을 보고 싶다

# 데이터 인덱싱, 필터링, 정렬, 변경 등

행/열 인덱싱: df.loc['행', '열']
열만 인덱싱: df.loc[:, 'mpg']
df.loc[0:3, ['mpg', 'cvl']] # 0, 1, 2, 3 포함

# 열 제거
df.drop(columns=['car','mpg'])

# 열 추가
df['new'] = df['mpg'] + 10

# 데이터 필터링
cond1 = (df['cyl'] == 1)
len(df[cond1])

df[ df['cyl']==4 ]: df 안에 cy1==4인 것들 뽑아줘

cond2 = (df['mpg'] >= 22)

# 2개 조건 필터링
and: df[cond1 & cond2]

or: df[cond1 | cond2]

# 데이터 정렬
df.sort_values('mpg', ascending=False) : 내림차순 정렬
df.sort_values('mpg', ascending=True) : 오름차순 정렬

# 데이터 변경(조건문)
# np.where 활용
# hp 변수 값중에서 205가 넘는 값은 205로 처리하고 나머지는 그대로 유지
df['hp'] = np.where(df['np'] >= 205, 205, df['hp'])


# 결측치, 이상치, 중복값 처리(제거 or 대체)

# 결측치 확인
df.isnull().sum()
# 결측치 제거
df.dropna(axis=0) 행 기준
df.dropna(subset=['칼럼']) # 이러면 칼럼에 있는 결측치 행만 제거
# 결측치 대체
df['age'] = df['age'].fillna(df['age'].mean())
df['age'] = df['age'].fillna(df['age'].mode()[0]) ## 최빈값은 [0] 필수!
df.isnull().sum()

# 이상치 확인 및 처리
Q1 = df['age'].quantile(0.25)
Q3 = df['age'].quantile(0.75)
IQR = Q3 - Q1

upper = Q3 + 1.5 * IQR
lower = Q1 - 1.5 * IQR

# 문제: age 변수의 이상치를 제외한 데이터 수는?
cond1 = (df['age'] <= upper) # 부호주의
cond2 = (df['age'] >= lower) # 부호주의
print(len(df[cond1 & cond2]))

# 표준정규분포 (이상치 +- 3Z값을 넘어가는 값) 이건 문제 마다 다름
# 데이터 표준화, Z = (개별값 - 평균) / 표준편차 Z-score

# 중복값을 제외하고 df1 변수에 넣어주세요(아직 나온 적 없음)
df1 = df.drop_duplicates()

# 데이터 scaling(데이터 표준화, 정규화)

# 표준화 StandardScaler = Z-score
from sklearn.preprocessing import StandardScaler

# 정규화
from sklearn.preprocessing import MinMaxScaler

# 데이터 합치기(아직 나온적 없음)
df_sum = pd.concat([df1, df2], axis = 0) # 행 방향으로 데이터 결합

# 날짜/시간 데이터, index 다루기 (1h 8min)
# 날짜 데이터 타입을 datetime으로 타입을 변경
# df['날짜'] = pd.to_datetime(df['날짜'])

# 년, 월, 일 변수(열) 추가하기
df['year'] = df['날짜'].dt.year
df['month'] = df['날짜'].dt.month
df['day'] = df['날짜'].dt.day

# 날짜 구간 필터링, 날짜 시간이 함께 있으면 날짜와 시간을 같이 써야된다.
df [df['날짜'].between('2023-01-01', '2023-02-23')] # 좌우 모두 포함
df [df['날짜'].between('2023-01-01 12:00:00', '2023-01-01 12:40:00')] # 좌우 모두 포함

# 날짜가 인덱스로 설정된 경우 아니면 안됨
df = df.set_index('날짜') drop=false 옵션은 인덱스로 설정하는데, 변수로도 남겨두겠다는거
df.loc['2023-01-05' : '2023-02-23']
df.loc[ (df.index >= '2023-01-05') & (df.indx<='2023-02-23') ]

# 시간 다루기
# 시간이 변수로 있는 경우 between 함수 사용 가능

# 시간이 index로 사용되는 경우
df.between_time(start_time='00:00:00', end_time='00:00:00') # 날짜와 상관없이 특정 시간대를 필터링 해야할 때, 시간이 인덱스에 위치해야함!!




====================================================

유형3 선형회귀 함수

# 유의한 변수는 P-value 0.05보다 작으면(상수항은 평가하지 않는다)

# 독립변수 설정
X = df[['Pclass', 'age', 'Parch']]
y = df['surivived']

# 모델링 Logit 함수
import statsmodels.api as sm

X = sm.add_constant(X) # 상수항 추가(조건에서 추가하지 말라고 하면 추가 x)
model_logit = sm.Logit(y, X).fit() # 주의!! Logit은 y, x순
summary = model_logit.summary()


# 만약 age변수가 5단위 증가하면 오즈비는 몇 배 증가?
result = np.exp(5 * model_logit.params)
print(round(result, 2))

==================================================
# 체험 문제 3유형 1번 문제
# Gender와 Survived 변수 간의 독립성 검정을 실시하였을 때, 카이제곱 통계량은?
from scipy.stats import chi2_contingency, ttest_1samp, ttest_ind, ttest_rel, chisquare
table = pd.crosstab(df['Gender'], df['Survived'])
print(chi2_contingency(table)) or chi2_contingency([df['Gender'], df['Survived']])

# 체험 문제 3유형 2번 문제
# Gender, SibSp, Parch를 독립변수로 사용하여 로지스틱 회귀모형을 실시하였을 때,
# Parch 변수의 계수값은?

from statsmodels.api import Logit, OLS
from statsmodels.formula.api import logit, ols

from sklearn.preprocessing import LabelEncoder
import statsmodels.api as sm

# 모델 생성시 object 자료형이 있으면 안되기 때문
encoder = LabelEncoder()
df['Gender'] = encoder.fit_transform(df['Gender'])

X = df[['Gender', 'SipSp', 'Parch']]
X = sm.add_constant(X) # 상수(Intercept, 절편) 값도 고려해야되기 때문
y = df['Survived']

model = Logit(y, X) # 순서 유의!!!
result = model.fit()
print(result.summary())


# 쉬운 풀이!! #########
# Logit(로지스틱 회귀모형), OLS(선형회귀 모형)
from statsmodels.api import Logit, OLS

formula = "Survived ~ Gender + SipSp + Parch"
results = Logit.from_formula(formula, df).fit() # 얘는 알아서 절편을 추가해줌, 범주형 변수도 알아서 계산해줌
print(results.summary())


# 3번 오즈비 구하기
print(np.exp(-0.3539))

===============================================================
# 수치형은 StandardScaler()
# object는 LabelEncoder(), 
# X_full = pd.get_dummies(X_full) one-hot 인코딩은 범주형 변수 알아서해줌, 칼럼 하나씩 지정해도 상관없음(df['age'])






--------------------------------------------------------------------------------------------
# 모델 평가지표 - 오차행렬
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, pred)

# 모델 평가지표 - 평가지표계산
from sklearn.metrics import classification_report
rpt = classification_report(y_test, pred)

# 정확도, 정밀도, 재현율, f1 score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=50, max_depth=3, random_state=20)

from sklearn.ensemble import RandomForestRegressor
rfr = RandomForestRegressor()
rfr.fit()

# 기울기, y 절편
rfr.coef_, rfr.intercept_



--------------------------------------------------------------------------------

